{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NetworkAnomalyDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-pAP94fkdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f55f153a-b500-4bff-a427-1279968d516a"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-2.4.4-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2J5hC82g67W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7imb7PUg86U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjnFUorJg_fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "from time import time\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkContext\n",
        "from pyspark.shell import sc\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row\n",
        "# from pyspark.sql.functions import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark.sql.functions as func\n",
        "import matplotlib.patches as mpatches\n",
        "from operator import add\n",
        "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
        "from operator import add\n",
        "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
        "from pyspark.mllib.util import MLUtils\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "import itertools\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Function definitions ------------------------------------------------------------------------------------------------\n",
        "# Read a comma delimited line of data\n",
        "def parse_line(line):\n",
        "    cols = line.split(',')\n",
        "    label = cols[-1]\n",
        "    vector = cols[:-1]\n",
        "    vector = [element for i, element in enumerate(vector) if i not in [1, 2, 3]]\n",
        "    vector = np.array(vector, dtype=np.float)\n",
        "    return (label, vector)\n",
        "\n",
        "# Get the 3 data fields that have the most variance\n",
        "def get_vars(data):\n",
        "    n = data.count()\n",
        "    means = data.reduce(add)/n\n",
        "    vars_ = data.map(lambda x: (x - means)**2).reduce(add)/n\n",
        "    return vars_\n",
        "\n",
        "# Euclidean distance function\n",
        "def euclidean_distance(a, b):\n",
        "    c = a - b\n",
        "    return np.sqrt(c.T.dot(c))\n",
        "\n",
        "# Get the clustering score of a potential k by evaluating the model with Within Set Sum of Squared Errors (WSSSE)\n",
        "def clustering_score(data, k):\n",
        "    clusters = KMeans.train(data, k, maxIterations=10, runs=10, initializationMode=\"random\")\n",
        "    WSSSE = clusters.computeCost(data)\n",
        "    return WSSSE\n",
        "\n",
        "# Apply a basic normalization function to the data to eliminate bias\n",
        "def normalize_data(data):\n",
        "    n = data.count()\n",
        "    means = data.reduce(add)/n\n",
        "    vars_ = data.map(lambda x: (x - means) ** 2).reduce(add)/n\n",
        "    stdevs = np.sqrt(vars_)\n",
        "    stdevs[stdevs == 0] = 1\n",
        "    # Normalize a single data point\n",
        "    def normalize(point):\n",
        "        return (point - means)/stdevs\n",
        "\n",
        "    return data.map(normalize)\n",
        "\n",
        "# Calculate WSSSE using a different method which determines closest centers with euclidean distance\n",
        "def get_WCSSE(clusters, data_point):\n",
        "    closest_centroid = clusters.centers[clusters.predict(data_point)]\n",
        "    return euclidean_distance(closest_centroid, data_point)**2\n",
        "\n",
        "# Load the data and begin preparing it for clustering and anomaly detection -------------------------------------------\n",
        "path = \"kddcup.data_10_percent\"\n",
        "data = sc.textFile(path, 12)\n",
        "\n",
        "# Parse the numerical data\n",
        "labeled_data = data.map(parse_line).cache()\n",
        "unlabeled_data = labeled_data.map(lambda row: row[1]).cache()\n",
        "# Get the total number of data points (should be ~5 million)\n",
        "n = unlabeled_data.count()\n",
        "print(\"# of connections: {}\".format(n))\n",
        "\n",
        "# Train the kmeans model\n",
        "t1 = time()\n",
        "clusters = KMeans.train(unlabeled_data, 2, maxIterations=10, runs=10, initializationMode=\"random\")\n",
        "print(\"Time elapsed during training: {}\".format(time() - t1))\n",
        "print\n",
        "# Select the 3 numerical data fields in the set with the most variance\n",
        "vars_ = get_vars(unlabeled_data)\n",
        "# Get the indices of the 3 data fields that vary the most\n",
        "indices = [t[0] for t in sorted(enumerate(vars_), key=lambda x: x[1])[-3:]]\n",
        "# Select a small amount of data to project and gather data points belonging to the two clusters\n",
        "data_to_project = unlabeled_data.randomSplit([10, 90])[0]\n",
        "rdd0 = unlabeled_data.filter(lambda point: clusters.predict(point)==0)\n",
        "rdd1 = unlabeled_data.filter(lambda point: clusters.predict(point)==1)\n",
        "# Store the centers of the clusters and take 5 data points from each one to project\n",
        "center0 = clusters.centers[0]\n",
        "center1 = clusters.centers[1]\n",
        "cluster0 = rdd0.take(5)\n",
        "cluster1 = rdd1.take(5)\n",
        "# Project the data\n",
        "cluster0_projected = np.array([[point[i] for i in indices] for point in cluster0])\n",
        "cluster1_projected = np.array([[point[i] for i in indices] for point in cluster1])\n",
        "# Take the min/max values from the clusters to use for the plot\n",
        "maximum = max(max(cluster1_projected.flatten()), max(cluster0_projected.flatten()))\n",
        "minimum = min(min(cluster1_projected.flatten()), min(cluster0_projected.flatten()))\n",
        "# Plot the clusters in a plot where each axis is bounded by the min/max values\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(cluster0_projected[:, 0], cluster0_projected[:, 1], cluster0_projected[:, 2], c=\"b\")\n",
        "ax.scatter(cluster1_projected[:, 0], cluster1_projected[:, 1], cluster1_projected[:, 2], c=\"r\")\n",
        "ax.set_xlim(minimum, maximum)\n",
        "ax.set_ylim(minimum, maximum)\n",
        "ax.set_zlim(minimum, maximum)\n",
        "ax.set_xlabel(\"duration\")\n",
        "ax.set_ylabel(\"src_bytes\")\n",
        "ax.set_zlabel(\"dst_bytes\")\n",
        "ax.legend([\"cluster 0\", \"cluster 1\"])\n",
        "plt.show()\n",
        "# Evaluate the trained model ------------------------------------------------------------------------------------------\n",
        "# Normalize the data to produce better clusters and figure out the ideal K --------------------------------------------\n",
        "# Retrieve the normalized data and set possible values for k as 60, 70, 80, 90, 100, 110\n",
        "normalized_data = normalize_data(unlabeled_data).cache()\n",
        "k_range = range(60, 111, 10)\n",
        "# Calculate the clustering scores in order to determine the best k (the lowest score is the best k)\n",
        "scores = [clustering_score(normalized_data, k) for k in k_range]\n",
        "plt.plot(k_range, scores)\n",
        "plt.xlabel(\"# of clusters\")\n",
        "plt.ylabel(\"Clustering score\")\n",
        "plt.show()\n",
        "# Use 90 as default k, but try to get the best one\n",
        "bestK = 90\n",
        "min_score = np.amin(scores)\n",
        "print(\"The min score is: {}\".format(min_score))\n",
        "for i in range(0, 5):\n",
        "    if scores[i] == min_score:\n",
        "        bestK = k_range[i]\n",
        "\n",
        "print(\"The best k is: {}\".format(bestK))\n",
        "# Select the data to project and train the model\n",
        "data_to_project = unlabeled_data.randomSplit([1, 999])[0].cache()\n",
        "clusters = KMeans.train(unlabeled_data, bestK, maxIterations=10, runs=10, initializationMode=\"random\")\n",
        "\n",
        "data_to_project_list = data_to_project.collect()\n",
        "data_projected = np.array([[point[i] for i in indices] for point in data_to_project_list])\n",
        "labels = [clusters.predict(point) for point in data_to_project_list]\n",
        "\n",
        "# Take the min and max again\n",
        "maximum = max(data_projected.flatten())\n",
        "minimum = min(data_projected.flatten())\n",
        "\n",
        "# Show the plot of the clusters before normalization\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(data_projected[:, 0], data_projected[:, 1], data_projected[:, 2], c=labels)\n",
        "ax.set_xlim(minimum, maximum)\n",
        "ax.set_ylim(minimum, maximum)\n",
        "ax.set_zlim(minimum, maximum)\n",
        "ax.set_xlabel(\"duration\")\n",
        "ax.set_ylabel(\"src_bytes\")\n",
        "ax.set_zlabel(\"dst_bytes\")\n",
        "ax.set_title(\"Clusters before normalization\")\n",
        "plt.show()\n",
        "\n",
        "# Train a new model with the normalized data\n",
        "clusters = KMeans.train(normalized_data, bestK, maxIterations=10, runs=10, initializationMode=\"random\")\n",
        "data_to_project_normed = normalize_data(unlabeled_data).cache()\n",
        "data_to_project_normed = data_to_project_normed.collect()\n",
        "data_projected = np.array([[point[i] for i in indices] for point in data_to_project_normed])\n",
        "labels = [clusters.predict(point) for point in data_to_project_normed]\n",
        "\n",
        "# Take the min and max for the normalized data\n",
        "maximum = max(data_projected.flatten())\n",
        "minimum = min(data_projected.flatten())\n",
        "\n",
        "# Plot the new clusters after normalization\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(data_projected[:, 0], data_projected[:, 1], data_projected[:, 2], c=labels)\n",
        "ax.set_xlim(minimum, maximum)\n",
        "ax.set_ylim(minimum, maximum)\n",
        "ax.set_zlim(minimum, maximum)\n",
        "ax.set_xlabel(\"duration\")\n",
        "ax.set_ylabel(\"src_bytes\")\n",
        "ax.set_zlabel(\"dst_bytes\")\n",
        "ax.set_title(\"Clusters after normalization\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}